

---
title: Dream of the Red Chamber
subtitle: Detection of Authorship
author: Xiang Ao
date: '2017-09-14'
slug: hong-lou-author-detection
categories:
  - R
tags:
  - regression

output:
  pdf_document:
    pandoc_args: [
      "--latex-engine","xelatex"
    ]
  html_document:
    toc: true
urlcolor: red
---

#  Introduction

In this post, I am trying to study the authorship of Dream of the Red
Chamber (https://en.wikipedia.org/wiki/Dream_of_the_Red_Chamber).  It
has been controvertial who the author of the last 40 chapters of this
most-well-known book in China.  Many people believed that the first 80 
chapters were written by Cao Xueqin, but the last 40 were additions by
someone else.

The original text was downloaded here: http://www.shuyaya.cc/book/2034/#download

I used R's packages "Rwordseg" to get tokens from the original text. I
rely on "cleanNLP" to get the term frequency matrix.  Then finally, in
round 3, I used "topicmodels" to detect authorship.

# Read in text and tokenize

```{r chunk1, warning=FALSE,  cache=TRUE}
# analysis starts here
library(rticles)
library(cleanNLP)
library(readr)
library(stringi)
library(ggplot2)
library(glmnet)
library(ggrepel)
library(viridis)
library(magrittr)

library(topicmodels)
library(tidyverse)
library(rJava)
library(Rwordseg)
library(RColorBrewer)
library(wordcloud)
library(tm)

require(readtext)
honglou1 <- readtext("~/projects/honglongmeng/honglou1.txt", text_field = "texts")

# here is to split into chapters using stringr's splitting functions
my_split <- function(text) {
    pattern <- '第.{1,3}回 '
    x <- str_split(text, pattern)[[1]]
    y <- str_extract_all(text, pattern)[[1]]
    data.frame(
        chapter = (1:length(x)) - 1,
        text = str_trim(x),
        header = c(y, "")
    )
}

chaps <- my_split(honglou1$text)


hong <- chaps %>%
    mutate(txt = as.character(text))

# use Rwordseg's segmentCN to get tokens from text.
honglou <- tbl_df(hong) %>%
     mutate(token=segmentCN(txt)) %>%
     mutate(id=chapter) %>%
     select(id, token)

```

#  Text analysis and author detection round 1

After generating tokens, I use cleanNLP\'s function called "get_tfidf"
to get the term-frequency matrix.  The basic idea is to see in each
chapter, how often does each term (token) apprear.

Then we use this matrix to detect authorship.  First I divide the
chapters into training and test samples, 50% and 50%.  Among the
training samples, if they belong to the first 80 chapters, I label the
author as "cao", otherwise, the label will be "unknown".  Then I train
the model on the training sample, and predict authorship on the test
sample.

I use package glmnet to do a elastic net (combination of lasso and
ridge).  Then use this model to predict authorship on the test sample.

We got the graph of probability of each chapter being written by Cao.  

```{r chunk2, warning=FALSE, cache=TRUE}

# make it long format: from each row for each chapter to one row for each token.
hongloumeng <- honglou %>%
    group_by(id) %>%
    unnest(token) %>%
    filter(id>0) %>%
    ungroup()

# get length (how many tokens in each chapter) on the way for each chapter
hongloumeng %>%
  group_by(id) %>%
  summarize(sent_len = n()) %$%
  quantile(sent_len, seq(0,1,0.1))

# frequency of tokens; get the top ones 
freq <- hongloumeng %>%
  group_by(token) %>%
  summarize(count = n()) %>%
  top_n(n = 100, count) %>%
  arrange(desc(count))

# set up the ususal format for text analysis
honglou2 <- hongloumeng %>%
    mutate(sid=1, tid=1, lemma=NA, upos=NA, pos=NA, cid=NA, word=token)

# assign first 80 chapters to Cao and the last 40 to unknown.
chapters <- honglou2 %>%
    group_by(id) %>%
    summarise(chapter=mean(id)) %>%
    mutate(author=ifelse(chapter<81, "cao", "unknown"))

# use cleanNLP's get_tfidf to get the term frequency matrix
# tfidf <- honglou2 %>%
#      get_tfidf(type = "tfidf", tf_weight = "dnorm", token_var="word")


#honglou_pca <- tidy_pca(tfidf$tfidf, chapters)

#ggplot(honglou_pca, aes(PC1, PC2)) +
#  geom_point(aes(color=chapter))


#mat.honglou <- get_tfidf(honglou2, min_df = 0.1, max_df = .9, type = "tf",
#                 tf_weight = "raw", doc_var = "id", token_var="word")

# a second specification for the term freqency matrix.
mat.honglou <- honglou2 %>%
    get_tfidf(type = "tf", tf_weight = "dnorm", token_var="word")

## tf2 <-  honglou2 %>%
##     get_tfidf(min_df = 0, max_df = 1, type = "tf",
##                  tf_weight = "raw",  token_var="word")

# random assign traing and testing samples
set.seed(1)
chapters <- chapters %>%
    mutate(training = as.logical(rbinom(length(chapter), 1, .5))) %>%
    mutate(y=as.numeric(author=="cao"))

chapters.train <- chapters %>% filter(training==1)
model <- cv.glmnet(mat.honglou$tf[chapters$training,], chapters.train$y,
                   family = "binomial", alpha=.9)

chapters$pred <- predict(model, newx=mat.honglou$tf, type='response', s=model$lambda.1se)

## ggplot(chapters, aes(chapter, pred)) +
##   geom_boxplot(aes(fill = author))

ggplot(chapters, aes(x = chapter, y = pred, color = training))  +geom_point() +geom_vline(xintercept=80) +geom_vline(xintercept=108) + geom_text(aes(chapter-3,pred, label = chapter), data = chapters %>% filter((pred<.85 & chapter<81 & training==0) | (pred > .5 & chapter>80 & training==0)))
```

In this graph, we have two vertical lines, Chapters 80 and 108.  It
was believed that first 80 chpaters are by Cao Xueqin, and the last 40
are not.  There is also belief that originally Cao's manuscript has
only 108 chapters.  

The two colors indicate whether they are in training, or test sample.
We can see that if the first 80 chapters were by Cao, then it's
unlikely the last 40 were by him too.  There are also a few chapters
in the first 80 chapters are not that consistent with Cao's style, for
example, chapters 10, 11, 60, 64, and 67.  Especially 67, very likely
it's not by Cao.  On the other hand, there are a few chapters in the
last 40 chapters that are possibly to be from Cao: chapters 84, 98,
and 119.



#  Text analysis and author detection round 2

In the last section, we labelled first 80 chapters\'s author as "Cao",
and last 40 chapters as "unknown".  Suppose we do know the first 80
are by Cao, but the last 40 we are not sure.  Let us randomly assign
authorship to each chapter for the last 40 chapters, 20\% by Cao and
80% by unknown author, then repeat this many time, to see each
chapter\'s probability of each author.  This way I introduce some
uncertainty about the last 40 chapters\'s authorship, instead of
labeling them first, then test it.




```{r  chunk3, warning=FALSE, cache=TRUE}

# here we introduce uncertainty: suppose the last 40 chapters have 20% chance been written by Cao.  

set.seed(1)
chapters.matrix <- matrix(NA,120,100)
training.matrix <- matrix(NA, 120, 100)
for (i in (1:100)){

chapters2 <- honglou2 %>%
    group_by(id) %>%
    summarise(chapter=mean(id)) %>%
    mutate(author=ifelse(chapter<81, "cao", ifelse(rbinom(40,1,.2),"cao","unknown"))) %>%
    mutate(training = as.logical(rbinom(length(chapter), 1, .5))) %>%
    mutate(y=as.numeric(author=="cao"))

chapters2.train <- chapters2 %>% filter(training==1)
model2 <- cv.glmnet(mat.honglou$tf[chapters2$training,], chapters2.train$y,
                   family = "binomial", alpha=.9)

chapters2$pred <- predict(model2, newx=mat.honglou$tf, type='response', s=model2$lambda.1se)

chapters.matrix[, i] <- chapters2$pred
training.matrix[,i] <- chapters2$training

}
## ggplot(chapters, aes(chapter, pred)) +
##   geom_boxplot(aes(fill = author))

wide <- bind_cols(data.frame(chapter=1:120), as.data.frame(chapters.matrix))
wide2 <- bind_cols(data.frame(chapter=1:120), as.data.frame(training.matrix))
long <- wide %>% gather("sim", "yhat",2:101)
long <- long %>% mutate(sim = as.integer(sub("V", "", sim)))
long2 <- wide2 %>% gather("sim", "training",2:101)
long2 <- long2 %>% mutate(sim = as.integer(sub("V", "", sim)))
long3 <- bind_cols(long, long2)[,c(1:3, 6)]

merged <- long3 %>% left_join(chapters2 %>% select(-training), by = "chapter")

#ggplot(merged, aes(y = yhat, x = chapter, group = chapter)) + geom_boxplot()
ggplot(merged, aes(y = yhat, x = chapter, group = chapter)) +
  geom_boxplot() + facet_grid(training ~ .)

```


#  Text analysis and author detection round 3

In the last section, we assigned some probability of different
authorships to the last 40 chapters.  What about we assign each
chapter some probability of authorship to each of the two authors,
then estimate these probabilities.  This seems to fall in the field of
topic models.


The only assumption we make here is that we have two authors.  Stictly
speaking, we assume there are two "topics" or "groups" for the 120
chapters.  Whether it's due to different authors, we don't know.  But
we assume authorship is the difference between two groups here.  LDA
(Linear Dirichlet Allocation) helps us calculating the probability of
each chapter belongs to each author.



```{r  chunk4, warning=FALSE, cache=TRUE}

set.seed(1)

mat.honglou2 <- honglou2 %>%
    get_tfidf(type = "tf", tf_weight = "raw", token_var="word",
    min_df=.05, max_df=.95)

lda2 <- LDA(mat.honglou2$tf, k = 2, method="Gibbs", control = list(seed = 1, burnin = 1000, thin = 100, iter = 1000))

plot(topics(lda2))
lda_topics <- as.data.frame(posterior(lda2)$topics)
names(lda_topics) <- c('prob1', 'prob2')

newdata <- bind_cols(chapters, lda_topics)
topics <- posterior(lda2)[["topics"]]
#heatmap(topics, scale = "none")

ggplot(newdata, aes(y = prob1, x = chapter, group = chapter)) +
 geom_point() +geom_vline(xintercept=80) +geom_vline(xintercept=108) + geom_text(aes(chapter-3,prob1, label = chapter), data = newdata %>% filter((prob1<.5 & chapter<81 ) | (prob1 > .5 & chapter>80 )))


```

We can see here 19 of the first 80 chapters have lower than 50% probability belonging to Author 1.  Strikingly, we found none of the last 40 chapters have higher than 50% chance belonging to Author 1, confirming that it's not the same author who wrote the last 40 chapters.  Meanwhile, Chapter 67 has only 25% chance written by Author 1 (presumably Cao Xueqin).






