---
title: 红楼梦 作者分析
author: Xiang Ao
date: '2017-09-19'
slug: red-chamber
categories:
  - R
tags:
  - plot
  - regression
  - R Markdown
---



<div class="section level1">
<h1>简介</h1>
<p>在本文中我们用机器学习和统计的方法来分析红楼梦的作者。红楼梦是中国最著名的小说之一(<a href="https://en.wikipedia.org/wiki/Dream_of_the_Red_Chamber" class="uri">https://en.wikipedia.org/wiki/Dream_of_the_Red_Chamber</a>)。 大多数人认为前八十回为曹雪芹所作， 后四十回为后人续作。 除非我们从考古中发现证据，唯一能告诉我们的便是小说本身。我们可以从文章的用词风格来判断作者。 即使续作者尽量模仿原作者的风格，也很难不在字里行间露出自己固有的风格。（我们也许可以用同样的方法来鉴定真画和假画，但是需要有原作者的大量画作）。</p>
<p>我是从这里下载的红楼梦原著120回版（具体哪个版本我也没仔细研究）: <a href="http://www.shuyaya.cc/book/2034/#download" class="uri">http://www.shuyaya.cc/book/2034/#download</a></p>
<p>我用了R中的 “Rwordseg” 来做中文分词。 就是把一句话分成词和词组。 然后用了 “cleanNLP” 得到分词频率矩阵 （“term frenquency matrix”）。 然后在最后一个模型用到 “topicmodels”。</p>
</div>
<div class="section level1">
<h1>读入文本进行分词</h1>
<p>这一部分就是读入文本， 把它分为120回，每一回作为一个文本， 然后分词。</p>
<pre class="r"><code># analysis starts here
library(rticles)
library(cleanNLP)
library(readr)
library(stringi)
library(ggplot2)
library(glmnet)
library(ggrepel)
library(viridis)
library(magrittr)

library(topicmodels)
library(tidyverse)
library(rJava)
library(Rwordseg)
library(RColorBrewer)
library(wordcloud)
library(tm)

require(readtext)
honglou1 &lt;- readtext(&quot;~/projects/honglongmeng/honglou1.txt&quot;, text_field = &quot;texts&quot;)

# here is to split into chapters using stringr&#39;s splitting functions
my_split &lt;- function(text) {
    pattern &lt;- &#39;第.{1,3}回 &#39;
    x &lt;- str_split(text, pattern)[[1]]
    y &lt;- str_extract_all(text, pattern)[[1]]
    data.frame(
        chapter = (1:length(x)) - 1,
        text = str_trim(x),
        header = c(y, &quot;&quot;)
    )
}

chaps &lt;- my_split(honglou1$text)


hong &lt;- chaps %&gt;%
    mutate(txt = as.character(text))

# use Rwordseg&#39;s segmentCN to get tokens from text.
honglou &lt;- tbl_df(hong) %&gt;%
     mutate(token=segmentCN(txt)) %&gt;%
     mutate(id=chapter) %&gt;%
     select(id, token)</code></pre>
</div>
<div class="section level1">
<h1>第一种模型</h1>
<p>生成分词以后， 我用“cleanNLP” 里的一个函数叫 “get_tfidf” 来取得分词频率矩阵。</p>
<p>首先我想到的办法是把所有章节分为“training” 和 “testing”。 这是机器学习的常用办法。 在“training” 章节中， 如果是前八十章， 我标识作者为 “Cao”， 否则标识为 “unknown”。</p>
<p>然后我用“elastic net” (combination of lasso and ridge) （一种机器学习方法）在 training sample。 然后在testing sample中来预测作者。</p>
<p>这样我们得到每一章作者是曹雪芹的概率。</p>
<pre class="r"><code># make it long format: from each row for each chapter to one row for each token.
hongloumeng &lt;- honglou %&gt;%
    group_by(id) %&gt;%
    unnest(token) %&gt;%
    filter(id&gt;0) %&gt;%
    ungroup()

# get length (how many tokens in each chapter) on the way for each chapter
hongloumeng %&gt;%
  group_by(id) %&gt;%
  summarize(sent_len = n()) %$%
  quantile(sent_len, seq(0,1,0.1))</code></pre>
<pre><code>##     0%    10%    20%    30%    40%    50%    60%    70%    80%    90% 
##  594.0 3229.6 3591.4 3852.2 4154.2 4311.5 4570.4 4750.0 5055.0 5744.5 
##   100% 
## 7114.0</code></pre>
<pre class="r"><code># frequency of tokens; get the top ones 
freq &lt;- hongloumeng %&gt;%
  group_by(token) %&gt;%
  summarize(count = n()) %&gt;%
  top_n(n = 100, count) %&gt;%
  arrange(desc(count))

# set up the ususal format for text analysis
honglou2 &lt;- hongloumeng %&gt;%
    mutate(sid=1, tid=1, lemma=NA, upos=NA, pos=NA, cid=NA, word=token)

# assign first 80 chapters to Cao and the last 40 to unknown.
chapters &lt;- honglou2 %&gt;%
    group_by(id) %&gt;%
    summarise(chapter=mean(id)) %&gt;%
    mutate(author=ifelse(chapter&lt;81, &quot;cao&quot;, &quot;unknown&quot;))

# use cleanNLP&#39;s get_tfidf to get the term frequency matrix
# tfidf &lt;- honglou2 %&gt;%
#      get_tfidf(type = &quot;tfidf&quot;, tf_weight = &quot;dnorm&quot;, token_var=&quot;word&quot;)


#honglou_pca &lt;- tidy_pca(tfidf$tfidf, chapters)

#ggplot(honglou_pca, aes(PC1, PC2)) +
#  geom_point(aes(color=chapter))


#mat.honglou &lt;- get_tfidf(honglou2, min_df = 0.1, max_df = .9, type = &quot;tf&quot;,
#                 tf_weight = &quot;raw&quot;, doc_var = &quot;id&quot;, token_var=&quot;word&quot;)

# a second specification for the term freqency matrix.
mat.honglou &lt;- honglou2 %&gt;%
    get_tfidf(type = &quot;tf&quot;, tf_weight = &quot;dnorm&quot;, token_var=&quot;word&quot;)

## tf2 &lt;-  honglou2 %&gt;%
##     get_tfidf(min_df = 0, max_df = 1, type = &quot;tf&quot;,
##                  tf_weight = &quot;raw&quot;,  token_var=&quot;word&quot;)

# random assign traing and testing samples
set.seed(1)
chapters &lt;- chapters %&gt;%
    mutate(training = as.logical(rbinom(length(chapter), 1, .5))) %&gt;%
    mutate(y=as.numeric(author==&quot;cao&quot;))

chapters.train &lt;- chapters %&gt;% filter(training==1)
model &lt;- cv.glmnet(mat.honglou$tf[chapters$training,], chapters.train$y,
                   family = &quot;binomial&quot;, alpha=.9)

chapters$pred &lt;- predict(model, newx=mat.honglou$tf, type=&#39;response&#39;, s=model$lambda.1se)

## ggplot(chapters, aes(chapter, pred)) +
##   geom_boxplot(aes(fill = author))

ggplot(chapters, aes(x = chapter, y = pred, color = training))  +geom_point() +geom_vline(xintercept=80) +geom_vline(xintercept=108) + geom_text(aes(chapter-3,pred, label = chapter), data = chapters %&gt;% filter((pred&lt;.85 &amp; chapter&lt;81 &amp; training==0) | (pred &gt; .5 &amp; chapter&gt;80 &amp; training==0)))</code></pre>
<p><img src="/post/2017-09-19-red-chamber_files/figure-html/chunk2-1.png" width="672" /></p>
<p>在图中， 我画了两条竖线， 80和108回。 有些人认为曹的原作只有108回。</p>
<p>两种不同颜色代表training or testing。 我们看到如果说前80回作者是“Cao”， 那么后40回作者不大可能是“Cao”. 甚至前80回中有几回也与其他章用词不太一致， 例如10, 11, 60, 64, 67。 特别是67, 不太可能作者是“Cao”。 在后40回中， 84,98,119 则与前80回风格较为一致。</p>
</div>
<div class="section level1">
<h1>第二种模型</h1>
<p>第二种模型是某种程度上的改善。 在第一种模型中， 我们把前80回标为“Cao”， 后40回为“unknown”。我们没有引入不确定性。 这里， 我们假设后40回有百分之二十可能作者是“Cao”。 这样我们引入不确定性。 我们还是用同一种统计模型elastic net。</p>
<pre class="r"><code># here we introduce uncertainty: suppose the last 40 chapters have 20% chance been written by Cao.  

set.seed(1)
chapters.matrix &lt;- matrix(NA,120,100)
training.matrix &lt;- matrix(NA, 120, 100)
for (i in (1:100)){

chapters2 &lt;- honglou2 %&gt;%
    group_by(id) %&gt;%
    summarise(chapter=mean(id)) %&gt;%
    mutate(author=ifelse(chapter&lt;81, &quot;cao&quot;, ifelse(rbinom(40,1,.2),&quot;cao&quot;,&quot;unknown&quot;))) %&gt;%
    mutate(training = as.logical(rbinom(length(chapter), 1, .5))) %&gt;%
    mutate(y=as.numeric(author==&quot;cao&quot;))

chapters2.train &lt;- chapters2 %&gt;% filter(training==1)
model2 &lt;- cv.glmnet(mat.honglou$tf[chapters2$training,], chapters2.train$y,
                   family = &quot;binomial&quot;, alpha=.9)

chapters2$pred &lt;- predict(model2, newx=mat.honglou$tf, type=&#39;response&#39;, s=model2$lambda.1se)

chapters.matrix[, i] &lt;- chapters2$pred
training.matrix[,i] &lt;- chapters2$training

}
## ggplot(chapters, aes(chapter, pred)) +
##   geom_boxplot(aes(fill = author))

wide &lt;- bind_cols(data.frame(chapter=1:120), as.data.frame(chapters.matrix))
wide2 &lt;- bind_cols(data.frame(chapter=1:120), as.data.frame(training.matrix))
long &lt;- wide %&gt;% gather(&quot;sim&quot;, &quot;yhat&quot;,2:101)
long &lt;- long %&gt;% mutate(sim = as.integer(sub(&quot;V&quot;, &quot;&quot;, sim)))
long2 &lt;- wide2 %&gt;% gather(&quot;sim&quot;, &quot;training&quot;,2:101)
long2 &lt;- long2 %&gt;% mutate(sim = as.integer(sub(&quot;V&quot;, &quot;&quot;, sim)))
long3 &lt;- bind_cols(long, long2)[,c(1:3, 6)]

merged &lt;- long3 %&gt;% left_join(chapters2 %&gt;% select(-training), by = &quot;chapter&quot;)

#ggplot(merged, aes(y = yhat, x = chapter, group = chapter)) + geom_boxplot()
ggplot(merged, aes(y = yhat, x = chapter, group = chapter)) +
  geom_boxplot() + facet_grid(training ~ .)</code></pre>
<p><img src="/post/2017-09-19-red-chamber_files/figure-html/chunk3-1.png" width="672" /></p>
<p>在这里我画的是重复100次取样后的box plot， 上面是testing, 下面是training。 第二种模型和第一种结论类似，只是引入不确定性。</p>
</div>
<div class="section level1">
<h1>第三种模型</h1>
<p>从前两次的模型中我想到也许应该用一种更好的模型， 就是每一章都有一定的概率属于“Cao”。 然后我们来估计这些概率。我发现这属于“topic models”。</p>
<p>现在我们唯一的假设是有两个作者， 严格说，是有两个”主题“，或者说，这120回可以分为两组，至于两组为什么不同，我姑且认为是作者不同，因为我们是从用词频率分析的。 也可能可以假设有三个作者，等等， 分析方法都一样。</p>
<p>这里我们用LDA(Linear Dirichlet Allocation) 分析每一章属于“Cao” 的概率。</p>
<pre class="r"><code>set.seed(1)

mat.honglou2 &lt;- honglou2 %&gt;%
    get_tfidf(type = &quot;tf&quot;, tf_weight = &quot;raw&quot;, token_var=&quot;word&quot;,
    min_df=.05, max_df=.95)

lda2 &lt;- LDA(mat.honglou2$tf, k = 2, method=&quot;Gibbs&quot;, control = list(seed = 1, burnin = 1000, thin = 100, iter = 1000))

plot(topics(lda2))</code></pre>
<p><img src="/post/2017-09-19-red-chamber_files/figure-html/chunk4-1.png" width="672" /></p>
<pre class="r"><code>lda_topics &lt;- as.data.frame(posterior(lda2)$topics)
names(lda_topics) &lt;- c(&#39;prob1&#39;, &#39;prob2&#39;)

newdata &lt;- bind_cols(chapters, lda_topics)
topics &lt;- posterior(lda2)[[&quot;topics&quot;]]
#heatmap(topics, scale = &quot;none&quot;)

ggplot(newdata, aes(y = prob1, x = chapter, group = chapter)) +
 geom_point() +geom_vline(xintercept=80) +geom_vline(xintercept=108) + geom_text(aes(chapter-3,prob1, label = chapter), data = newdata %&gt;% filter((prob1&lt;.5 &amp; chapter&lt;81 ) | (prob1 &gt; .5 &amp; chapter&gt;80 )))</code></pre>
<p><img src="/post/2017-09-19-red-chamber_files/figure-html/chunk4-2.png" width="672" /></p>
<p>我们看到前80回有19回是作者1写的概率低于50%。 尤其第67回， 只有25%的概率是作者1写的。 在后40回中， 没有一回概率大于50%！ 作者1应该是曹雪芹。这里我们不能分析出作者是谁，只是说后40回的作者与前80回大多数章节的作者不同。</p>
</div>
